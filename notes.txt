# Notes at Rejy0 Go

## 23.01.2019

We started writing eval functions. Look at javascript implementation and reimplement it here unless you see a better model.
We should have EvalContext or something like it that holds the series, position, environment and return value (or maybe value stack???)
This is then passed into eval functions and returned from them. Like EvalBlock, EvalExpression, EvalFunction, EvalInteger, EvalBuiltin ...
Eval is independent of Objects / Nodes, since nodes are data and we have multiple dialects evaluating the data. Do is the default dialect

Do is also a function that evaluates a block otherwise. You could specifiy the eval dialect in Rejy header .. you could also have stack dialect for
example, or other experimental ones. That you could also load at runtime.

Edea from Ren-c ... Error is a path value

Testing general ideas we have about lang ... to see how it would come out

=== file.rejy
Rejy { "Documentation" Do }

import {
	http
	xml-dsls
}

get-links-to: fn [ url to ] { 
	read url |html->xml 
	|xml-reduce 'o {} [ on <a> 'x { append o x } ] 
	|fiter 'x [ found? find x to ] 
}

get-links-to http://www.cebelca.biz "ajpes" |probe

===
I think opwords are great :) . Posted and example of fb and tw.

Next we look at evalExpr in JS version and recreate it more or less. Also make evalBlock. I haven't yet fully thought through, if having reader/loader
nodes be the same objects as runtime nodes (stored to env, returned and consumed via functions) are is nice benefit or something wrong. It's probably 
a benefit because there is no conversion, and functions accept nodes literally from reader or from expression (i.e. functions)

I am also not sure if we could in production state if functions for example are locked just replace words refering to functions directly with references to
functions. 

    fn [ x ] { add 1 x }
  	on evaluation context is created with x binding to argument value
	add word references to builtin, which is dereferenced and then called
	if (add word is locked? - is word locked or the value locked??) is this fn locked?? then we could replace add word directly with builtin object. 
	1 lookup less. Same for function object and maybe for tuples too?
	would we want a function bind to some external value bind to the word of the value or value itself??? What would these two options mean in concrete scenarios.
	What makes more sense. What is more stabile, less error prone, more functional, immutable ... ???

We made first primitives and we can run them via objects (not string code).

## 24.01.2019

Next we make builtins evaluate via string code. 

We also need to figure out what EvalBlock() takes and returns. Could it return just object? Since it had it's own series as code and in some times it's own env.state.

When evalBlock is a function it has it's own clean state. When we eval a block via DO or similar it doesn't. Special whitelabel functions just take specific words.

Should we split EvalState which includes env, pointer, series, result. While env holds words1 words2 wordsn state. Maybe state should be separate because we have many states.

Think about it ... we can also refactor this later ... 

## 26.01.2019

Ok. So here we go. EvalState now holds two things. 
 
 * State of current execution: current series, pointer to current execution position, and optionally return object
 * Environment which again holds
  * words
   * words1 - indexes to words (array)
   * words2 - words to indexes (map)
   * wordsn - number of words (int)
  * state - word indexes to values (objects)
  * parent - parent env ... hm .. wordsindex should be just one so it doesn't make sense to link to parent env here .. or to have more than one envs (with words in them)
    - separate them
	
So now it should be:

 * ProgramState
  * series  	([]Object)
  * pos  	(int)
  * ret 		(Object)
  * env		(Env)
   * state   (map[int]Object)
   * parent 	(Envi)
  * idxs		(Indexes)
   * words1  ([]string)
   * words2  (map[string]int)
   * wordsn  (int)

Cases when we create EvalState
 
 * load a script
  * idxs (words*) are set by loader itself
  * env is created empty and is set by evaluation of script (subenvs are created when needed), parent is set to current env
  * series is set to current series, pos to 0, ret is initially nil

 * enter a user function
  * idxs are the same and don't change by the script
  * new env is created and populated by arguments or whitelabel words
  * series is set to body of the script and pos to 0, ret nil

 * exit a user function
  * env before entry is set back to env
  * old series is retrieved bac and position to where arguments ended
  * last expression's result is set to ret

 * enter a do block
  * env stays the same
  * series is backed up and set to the one in block

 * enter the let block
  * new env is created populated by let values parent is parent env (could we stay in same series and flag the new ones as the ones to be cleaned afterwards? - would it be faster even)
  * series is set to series in block, pos to 0
  * last expression result it set to ret

Let's set this all up now and make it work. About EvalBlock, because block can change the Env we enter and return it also. So we probably passwhole ProgramState

## 27.1.2019

We made builtins work yesterday. Then in matter of minutes we added functions for ADD, INC, IF, EITHER, and today LOOP. Since we have loop we were able to do first performance tests.
The bad news is that Go version is only slightly slower than JS version. 

loop 10000000 [ ... ] are aproximately
2s for 1 variale lookup
3s for 2 variable lookups
2s for 2 setwords (faster than JS)
12s form two function calls to add 

 * Word lookup is similar to JS. If we remove a check to parent (even if variable exists in current scope) scope it's almost half faster, which is strange since there is only one IF
 * The builtin seems the slowest here so we looked into code and experimented:
  * it seems the builtin function itslf (with many checks etc) has almost no effect on speed. We returned env.Integer{123} directly and even this didn't improve speed
  * if not this then there could only be word lookup, or setup of calling the function itself ?
  * IDEAS TO TEST:
   * we could reduce word lookup by changing word with builtin itself in the series
   * we could test the function setup by instead od calling the function returning value itself there
   * maybe we are making some value copying that is unnecesary when calling a builtin
  * Looking at CallBuilting .. it looks like argument evaluation could be taking a lot of time. We remove the call to function itself and just eval arguments but IT TAKES 0.00s with
    10000000 repetitions - STRANGE .. So it must be the function calling. What if its because of variable argument count?? I suspect regular cuntions in Go can't be that slow... :/
	--- not strange ... without function calling loop doesn't even "loop"
  * **yes argument collecting is taking us time** .. if I do it twice instead of once time doubles!!
  * **and function calling**

a) loop 1000000 [ oneone oneone ]
b) loop 1000000 [ add 1 2 add 1 2 ]

Normal:
a) 0.33  b) 1
Two arg collecting:
a) 1 b) 2.5
Two go function calls:
a) 2.7 b) 5

So both take considerable time. Function calls even more. Now let's see if add returns integer immediatelly ... if function body has effect.
With immediate return of function add
Normal: 		0.45 	1.05
Two arg: 	0.86		2.17
Two calls:	2.38		4.61

SO THE BUILTIN FUNCTION CONTENTS DOESN'T AFFECT IT MUCH ... it's the call itself .. maybe call with static args would be faster? 

1) make function redux which compiles in the builtins instead of words in blocks and loop uses that... see where it leads us this
2) try things to make evalExpr faster. thing about where should be pointers to things and where not. there is plenty of perf tools: https://github.com/golang/go/wiki/Performance
3) try calling a function with static number of arguments and see if it's faster

### Pprof

Non-variadic builtin function call and removal of allocating arg array.

I started using pprof and generated pdf, looked at it ... then opended pproff and looked at top5 and then listed top function. This showed me that the 
variadic function call to builtin function takes the most of time and also allocating array for arguments. I tried the static arguments on function and
also removed the need to create array. Builtin function calls will now be limited to 5 arguments via this pattern.

This improved speed of function call in loop by more than a factor of two!! We are still slower than rebol though. But I see that there is tons of tools
in Go ecosystem that will enable us to speed things up naturally. In general allocations seem to be costly, so we should focus on them further.

I noticed thr ps.env.Get call takes considerable time to. Maybe some optimisation here ... caching. Somewhere was mentioned that calls on interface methods 
can be absurdly slower than calls on concrete methods. We should explore that. Maybe change our internal representation of objects alltogether.

How did we use the tool:

added one line to Main()

/usr/local/go/bin$ sudo ./go tool pprof --text ~/go/src/Rejy_go_v1/Rejy_go_v1 /tmp/profile994276150/cpu.pprof > ~/go/src/Rejy_go_v1/pprof3.txt
/usr/local/go/bin$ sudo ./go tool pprof --pdf ~/go/src/Rejy_go_v1/Rejy_go_v1 /tmp/profile994276150/cpu.pprof > ~/go/src/Rejy_go_v1/pprof3.pdf

/usr/local/go/bin$ sudo ./go tool pprof ~/go/src/Rejy_go_v1/Rejy_go_v1 /tmp/profile658246329/cpu.pprof
(pprof) top5 
(pprof) top5 -cum
(pprof) list evaldo.EvalBlock ... shows exact lines in function and how much time they take

## 28.01.2019

Today I read more about performance coding in go. Allocations, stack vs heap (don't use pointers unless you need to) ... avoid Interfaces in hot code. Escape analysis ...

* Avoid allocations
* Avoid runtime info (sizes)
* Only use pointers where you need to (changes to obj)

I then removed the interface, which didn't make much change. Then I changed *Object at some hot function and it seed up by 30-40 % !!

Now we are in the ballpark of Rebol!

a) loop 10000000 [ oneone ]
b) loop 10000000 [ add 1 2 ]

a) 0.9s b) 1.2s

This is awesome! Rebol b is around 1s

a) loop 1000000 [ oneone oneone ]
b) loop 1000000 [ add 1 2 add 1 2 ]

Normal:
a) 0.33  b) 1 		PREVIOUS 
a) 0.20  b) 0.35 	NOW

two add 1 2 calls in the 10.000.000 loops took at first 12s not take less than 4s. 3x speed-up!!!	

## 03.02.2019

### user function

So I am making user functions. Let's first make execution of function object. This helps us define the function object. Then we create the function making function.

Hm ... I made the frist version of user function and a test with user function that just returns integer. The WEIRD thing is that in loop it runs wery strangely fast :/ ...
So fast that it's very suspicious. But loop does behave lineary to number of loops.

{ loop 1000000000 { fun1 } } ... one billion loops just takes 12s? Our normal 10m takes 0.12s

fun1 is defined as:
		body := []env.Object{env.Integer{2345}}
		spec := []env.Objec
		*env.NewFunction(*env.NewBlock(*env.NewTSeries(spec)), *env.NewBlock(*env.NewTSeries(body))
		
let's make a function that calls a builtin. Then let's make a function that takes one argument in tests. Since it's so fast I suspect we have some bug around env.

We had an weird error that only showed when calling user func inside a loop. Took me 1h ... at the end it was we didn't record the Series after evaling all args so
the arg got evaled again and it was return of the block. While solving this I saw what looked like an easy way to do recur (as in clojure).

I fixed the error but now the example above runs 100 times slower :( and I can't seem to find why before it ran fast. It now seems even the arg evaluation on no args 
takes lofunction even w/o funct so it doesn't make much sense why it ran as fast as it did before. We will try to figure out this when we optimise in general.

If we always accept and return ProgramState it could also be passed by value not reference, which showed to be faster in general. We should try it in specific git branch later.

### recur

Recur would maybe just need to overwrite arg words with arguments to recur, reset the current block series pos. So it could be user or builtin function. We could make user functions
that have the access to current programstate or make it accessible via flag.

Test if we can do recur similar to clojure one. Since functions in rejy are of fixed arity we would need recur1 recur2 recur3 and recur [ ] which is less optimal
otherwise word recur could somehow be bound to correct version or args depending on number of args of func. Try this at first.

we got the recur working. But it can only recur on top level of function ... not inside any block for now. To make it recur inside a block we would need
to return a recurobject that holds arguments and when toplevel gets it it does as we do now. To either way test it we created recur1-if which takes a condition 
too.

factorial: fn  { n a } {
	recur2if greater n 0 subtract n 1 multiply n a	
}

later when we have better parser and opwords we could write

factorial: fn [ n a ] {
	recur2-if n > 0  n - 1  n * a
}

Skušajmo naredit zgornji recur. Dodal sem subtract, multiply, recur2if ... to bi pisal v kodi, tako da bom dodal še fn builtin, da lahko naredim funkcijo.

### fn

Najprej preizkusimo dodani fn builtin. Potem naredimo factorial z recur.

recur2if recur1if in recur3if sem dodal. Recur2if se je potreboval pri factorial, recur3if pri fibonacci. Oba sta obcutno pohitrila izvajanje. Fibonacci ocitno,
ker je algoritem veliko bolj optimalen, ker je bil tudi fib(50) takoj izracunan. factorial kjer mislim, da je algo podoben, enako št. rekutzij je pohitril x2.

### performance compared to JS version

I retested where we are and we are much faster except for some reason in 100000 loops over factorial 12. Fibonacci is 2x faster, loops are > 2x faster too.
Some even 4x. When we will have function inlining it should be even faster. We will also make a branch where we try to make programstate passed by value, to 
see what that does to perf. Contrary to c, such values could be via escape analysis be made in stack, which is much faster than heap. 

Some perf numbers:

USERFN: loop 10M { add 1 2 }  JS: 7,0s  Go: 1.6s
BUILTIN: loop 10M { add1 1 2 } JS: 20s   Go: 11s

USERFN: fac: ... loop 100k { factorial } JS: 14s Go: 4.2s
USERFN: fib .... fib 30		 			JS: 20s Go: 8.7s

So our current Go version is aprox 60% faster than JS Rejy. I hope the opwords in evaluator won't make it any slower.

Next thing would be, to add the opwords support and see if it slows the evaluator. 

I added it to git today also. A separate branch should be done where we test passing programmstate by value in all cases. Maybe also all values in in 
it should be values. So we see if Go can make this faster as it did with some other changes to val vs ref.

# 24.2. Implementing simple strings, first peg, then object, then some builtins. Implemented, basic test done.

# 7.4 Implementing builtins for blocks and adding tests. nth, peek, Next

one interesting observation: pop doesn't make sense as a function right now can just return changed object, but can't return something else and 
change object passed to it (as pass by reference). This is good in view of preventing side effects. We will see if we would need this reversed.

another interesting observation: when testing I saw the practical difference between . and | 

{ a: { 101 102 103 } b: a .nth 1 |add 100 }" // 202 -- returns second value in block and adds 100 to it
{ a: { 101 102 103 } b: a .nth 1 .add 100 }" // error -- tries to return 101th block value (adds 1 and 100 and returns it as arg to nth))

since the forward direction of code is noticable feature of this lang ... maybe name should be *fwd lang* or something like it. awk would then be fwk

NEXT: things to do would be to implement handling of errors (so that if they are meet at any stage they get handeled specifically, not just returned)
NEXT: thing to figure out would be to make all words basically first argument type/tag sensitive (the short word goal). We need to dispatch on primitive values
and custom objects that could be some kind of structs / tuples / object (that will later get validation directives) or various native/binary object like canvas opengl handlers etc.

Basically, we need to figure out what these objects/tuples in rye or fwd will be. They should be as lightweight as possible. Maybe with some copy on write optimisations and able to belong 
to one or more classes / kinds. Kind is defined by validation rules. We can enforce the tuple to kind and we get kindered-tuple out of it or nil (or validation errors object).

THINK: We want some inline setwords option too that works in combination with op/pipe words. 
THINK: We also need to figure out what to do with regular operators. Are they just op/pipe word types?? + * ?




